
  0%|                                                                                                                                                                                             | 0/4 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.64it/s]
  0%|                                                                                                                                                                                             | 0/1 [00:00<?, ?it/s]

{'eval_loss': 16.459901809692383, 'eval_runtime': 0.471, 'eval_samples_per_second': 8.492, 'eval_steps_per_second': 2.123, 'epoch': 1.0}
There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.86s/it]
  0%|                                                                                                                                                                                             | 0/1 [00:00<?, ?it/s]
{'train_runtime': 9.5633, 'train_samples_per_second': 0.418, 'train_steps_per_second': 0.418, 'train_loss': 10.020528793334961, 'epoch': 1.0}

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.34it/s]