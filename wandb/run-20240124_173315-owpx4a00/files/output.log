
  0%|                                                                                                                                                                                                                 | 0/20 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

 20%|████████████████████████████████████████▏                                                                                                                                                                | 4/20 [00:06<00:15,  1.05it/s]

 40%|████████████████████████████████████████████████████████████████████████████████▍                                                                                                                        | 8/20 [00:11<00:09,  1.21it/s]
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                | 12/20 [00:15<00:06,  1.23it/s]
  0%|                                                                                                                                                                                                                  | 0/1 [00:00<?, ?it/s]


 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 15/20 [00:19<00:05,  1.01s/it]



100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:24<00:00,  1.28it/s]
{'eval_loss': 6.394824504852295, 'eval_runtime': 0.5067, 'eval_samples_per_second': 7.894, 'eval_steps_per_second': 1.973, 'epoch': 5.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:24<00:00,  1.28it/s]There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:27<00:00,  1.39s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.32it/s]
[32m[I 2024-01-24 17:33:44,239][39m Trial 0 finished with value: 6.4433722496032715 and parameters: {'attention_probs_dropout_prob': 0.016261301011788975, 'learning_rate': 6.7487089067981e-05, 'weight_decay': 0.0018838334811845168}. Best is trial 0 with value: 6.4433722496032715.
[34m[1mwandb[39m[22m: [32m[41mERROR[39m[49m Attempted to change value of key "attention_probs_dropout_prob" from [0.016261301011788975] to 0.016261301011788975
[34m[1mwandb[39m[22m: [32m[41mERROR[39m[49m If you really want to do this, pass allow_val_change=True to config.update()
Traceback (most recent call last):
  File "rev_trials.py", line 196, in <module>
    run_trials(data_path=data_paths[i],
  File "rev_trials.py", line 152, in run_trials
    study.optimize(func=lambda trial: objective(trial, training_args, roberta_config), n_trials=5, callbacks=[wandbc])
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/study/_optimize.py", line 174, in _optimize_sequential
    callback(study, frozen_trial)
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/integration/wandb.py", line 173, in __call__
    run.config.update({**attributes, **trial.params})
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 184, in update
    sanitized = self._update(d, allow_val_change)
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 177, in _update
    sanitized = self._sanitize_dict(
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 237, in _sanitize_dict
    k, v = self._sanitize(k, v, allow_val_change)
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 258, in _sanitize
    raise config_util.ConfigError(
wandb.sdk.lib.config_util.ConfigError: Attempted to change value of key "attention_probs_dropout_prob" from [0.016261301011788975] to 0.016261301011788975
If you really want to do this, pass allow_val_change=True to config.update()
Traceback (most recent call last):
  File "rev_trials.py", line 196, in <module>
    run_trials(data_path=data_paths[i],
  File "rev_trials.py", line 152, in run_trials
    study.optimize(func=lambda trial: objective(trial, training_args, roberta_config), n_trials=5, callbacks=[wandbc])
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/study/_optimize.py", line 174, in _optimize_sequential
    callback(study, frozen_trial)
  File "/chronos_data/ssmith/lib/python3.8/site-packages/optuna/integration/wandb.py", line 173, in __call__
    run.config.update({**attributes, **trial.params})
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 184, in update
    sanitized = self._update(d, allow_val_change)
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 177, in _update
    sanitized = self._sanitize_dict(
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 237, in _sanitize_dict
    k, v = self._sanitize(k, v, allow_val_change)
  File "/home/ssmith/.local/lib/python3.8/site-packages/wandb/sdk/wandb_config.py", line 258, in _sanitize
    raise config_util.ConfigError(
wandb.sdk.lib.config_util.ConfigError: Attempted to change value of key "attention_probs_dropout_prob" from [0.016261301011788975] to 0.016261301011788975
If you really want to do this, pass allow_val_change=True to config.update()