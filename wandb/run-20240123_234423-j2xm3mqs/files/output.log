
  0%|                                                                                                                                                                                                                 | 0/20 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

 20%|████████████████████████████████████████▏                                                                                                                                                                | 4/20 [00:04<00:11,  1.42it/s]

 25%|██████████████████████████████████████████████████▎                                                                                                                                                      | 5/20 [00:07<00:24,  1.64s/it]


 50%|████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                    | 10/20 [00:11<00:10,  1.03s/it]


 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                            | 14/20 [00:15<00:06,  1.08s/it]


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:21<00:00,  1.33it/s]
{'eval_loss': 9.110822677612305, 'eval_runtime': 0.5025, 'eval_samples_per_second': 7.961, 'eval_steps_per_second': 1.99, 'epoch': 5.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:21<00:00,  1.33it/s]There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:24<00:00,  1.21s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.37it/s]
[32m[I 2024-01-23 23:44:48,593][39m Trial 0 finished with value: 9.030842781066895 and parameters: {'attention_probs_dropout_prob': 0.2866541774330238, 'learning_rate': 0.011332582191758185, 'weight_decay': 0.002062021218525216}. Best is trial 0 with value: 9.030842781066895.